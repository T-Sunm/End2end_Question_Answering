{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "    \n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import evaluate\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sử dụng mô hình \"distilbert-base-uncased\"\n",
    "# làm mô hình checkpoint\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "# Độ dài tối đa cho mỗi đoạn văn bản\n",
    "# sau khi được xử lý\n",
    "MAX_LENGTH = 384\n",
    "\n",
    "# Khoảng cách giữa các điểm bắt đầu\n",
    "# của các đoạn văn bản liên tiếp\n",
    "STRIDE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 87599/87599 [00:00<00:00, 633089.81 examples/s]\n",
      "Generating validation split: 100%|██████████| 10570/10570 [00:00<00:00, 612649.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME = \"squad\"\n",
    "raw_datasets = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 87599/87599 [00:01<00:00, 52579.21 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 0\n",
       "})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 6549, 2135, 1010, 1996, 2082, 2038, 1037, 3234, 2839, 1012, 10234, 1996, 2364, 2311, 1005, 1055, 2751, 8514, 2003, 1037, 3585, 6231, 1997, 1996, 6261, 2984, 1012, 3202, 1999, 2392, 1997, 1996, 2364, 2311, 1998, 5307, 2009, 1010, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 6924, 2007, 1996, 5722, 1000, 2310, 3490, 2618, 4748, 2033, 18168, 5267, 1000, 1012, 2279, 2000, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 102], [101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 2364, 2311, 1998, 5307, 2009, 1010, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 6924, 2007, 1996, 5722, 1000, 2310, 3490, 2618, 4748, 2033, 18168, 5267, 1000, 1012, 2279, 2000, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 14042, 2173, 1997, 7083, 1998, 9185, 1012, 2009, 2003, 1037, 15059, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 102], [101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 14042, 2173, 1997, 7083, 1998, 9185, 1012, 2009, 2003, 1037, 15059, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271, 1999, 8517, 1012, 2012, 1996, 2203, 1997, 1996, 2364, 3298, 1006, 1998, 1999, 1037, 3622, 2240, 2008, 8539, 2083, 1017, 11342, 1998, 1996, 2751, 8514, 102], [101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271, 1999, 8517, 1012, 2012, 1996, 2203, 1997, 1996, 2364, 3298, 1006, 1998, 1999, 1037, 3622, 2240, 2008, 8539, 2083, 1017, 11342, 1998, 1996, 2751, 8514, 1007, 1010, 2003, 1037, 3722, 1010, 2715, 2962, 6231, 1997, 2984, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'offset_mapping': [[(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (71, 72), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (251, 253), (253, 255), (255, 257), (258, 260), (261, 263), (264, 266), (266, 269), (270, 271), (271, 272), (273, 277), (278, 280), (281, 284), (285, 289), (290, 298), (299, 301), (302, 305), (306, 314), (315, 317), (318, 321), (322, 328), (329, 334), (334, 335), (336, 347), (348, 354), (355, 358), (359, 367), (368, 370), (371, 374), (375, 377), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (71, 72), (0, 0), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (251, 253), (253, 255), (255, 257), (258, 260), (261, 263), (264, 266), (266, 269), (270, 271), (271, 272), (273, 277), (278, 280), (281, 284), (285, 289), (290, 298), (299, 301), (302, 305), (306, 314), (315, 317), (318, 321), (322, 328), (329, 334), (334, 335), (336, 347), (348, 354), (355, 358), (359, 367), (368, 370), (371, 374), (375, 377), (377, 381), (381, 382), (383, 384), (385, 391), (392, 397), (398, 400), (401, 407), (408, 411), (412, 422), (422, 423), (424, 426), (427, 429), (430, 431), (432, 439), (440, 442), (443, 446), (447, 449), (449, 453), (454, 456), (457, 460), (460, 464), (464, 465), (466, 472), (473, 478), (479, 482), (483, 489), (490, 494), (495, 502), (502, 504), (505, 513), (514, 516), (517, 522), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (71, 72), (0, 0), (281, 284), (285, 289), (290, 298), (299, 301), (302, 305), (306, 314), (315, 317), (318, 321), (322, 328), (329, 334), (334, 335), (336, 347), (348, 354), (355, 358), (359, 367), (368, 370), (371, 374), (375, 377), (377, 381), (381, 382), (383, 384), (385, 391), (392, 397), (398, 400), (401, 407), (408, 411), (412, 422), (422, 423), (424, 426), (427, 429), (430, 431), (432, 439), (440, 442), (443, 446), (447, 449), (449, 453), (454, 456), (457, 460), (460, 464), (464, 465), (466, 472), (473, 478), (479, 482), (483, 489), (490, 494), (495, 502), (502, 504), (505, 513), (514, 516), (517, 522), (523, 527), (527, 530), (530, 533), (534, 536), (536, 538), (538, 541), (541, 543), (544, 546), (547, 551), (551, 552), (553, 555), (556, 559), (560, 563), (564, 566), (567, 570), (571, 575), (576, 581), (582, 583), (584, 587), (588, 590), (591, 592), (593, 599), (600, 604), (605, 609), (610, 618), (619, 626), (627, 628), (629, 636), (637, 640), (641, 644), (645, 649), (650, 654), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (71, 72), (0, 0), (440, 442), (443, 446), (447, 449), (449, 453), (454, 456), (457, 460), (460, 464), (464, 465), (466, 472), (473, 478), (479, 482), (483, 489), (490, 494), (495, 502), (502, 504), (505, 513), (514, 516), (517, 522), (523, 527), (527, 530), (530, 533), (534, 536), (536, 538), (538, 541), (541, 543), (544, 546), (547, 551), (551, 552), (553, 555), (556, 559), (560, 563), (564, 566), (567, 570), (571, 575), (576, 581), (582, 583), (584, 587), (588, 590), (591, 592), (593, 599), (600, 604), (605, 609), (610, 618), (619, 626), (627, 628), (629, 636), (637, 640), (641, 644), (645, 649), (650, 654), (655, 656), (656, 657), (658, 660), (661, 662), (663, 669), (669, 670), (671, 677), (678, 683), (684, 690), (691, 693), (694, 698), (698, 699), (0, 0)]], 'overflow_to_sample_mapping': [0, 0, 0, 0]}\n",
      "inputs_id:  [[101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 6549, 2135, 1010, 1996, 2082, 2038, 1037, 3234, 2839, 1012, 10234, 1996, 2364, 2311, 1005, 1055, 2751, 8514, 2003, 1037, 3585, 6231, 1997, 1996, 6261, 2984, 1012, 3202, 1999, 2392, 1997, 1996, 2364, 2311, 1998, 5307, 2009, 1010, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 6924, 2007, 1996, 5722, 1000, 2310, 3490, 2618, 4748, 2033, 18168, 5267, 1000, 1012, 2279, 2000, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 102], [101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 2364, 2311, 1998, 5307, 2009, 1010, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 6924, 2007, 1996, 5722, 1000, 2310, 3490, 2618, 4748, 2033, 18168, 5267, 1000, 1012, 2279, 2000, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 14042, 2173, 1997, 7083, 1998, 9185, 1012, 2009, 2003, 1037, 15059, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 102], [101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 14042, 2173, 1997, 7083, 1998, 9185, 1012, 2009, 2003, 1037, 15059, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271, 1999, 8517, 1012, 2012, 1996, 2203, 1997, 1996, 2364, 3298, 1006, 1998, 1999, 1037, 3622, 2240, 2008, 8539, 2083, 1017, 11342, 1998, 1996, 2751, 8514, 102], [101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271, 1999, 8517, 1012, 2012, 1996, 2203, 1997, 1996, 2364, 3298, 1006, 1998, 1999, 1037, 3622, 2240, 2008, 8539, 2083, 1017, 11342, 1998, 1996, 2751, 8514, 1007, 1010, 2003, 1037, 3722, 1010, 2715, 2962, 6231, 1997, 2984, 1012, 102]]\n",
      "overflow_to_sample_mapping:  [0, 0, 0, 0]\n",
      "off_mapping:  [[(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (71, 72), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (251, 253), (253, 255), (255, 257), (258, 260), (261, 263), (264, 266), (266, 269), (270, 271), (271, 272), (273, 277), (278, 280), (281, 284), (285, 289), (290, 298), (299, 301), (302, 305), (306, 314), (315, 317), (318, 321), (322, 328), (329, 334), (334, 335), (336, 347), (348, 354), (355, 358), (359, 367), (368, 370), (371, 374), (375, 377), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (71, 72), (0, 0), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (251, 253), (253, 255), (255, 257), (258, 260), (261, 263), (264, 266), (266, 269), (270, 271), (271, 272), (273, 277), (278, 280), (281, 284), (285, 289), (290, 298), (299, 301), (302, 305), (306, 314), (315, 317), (318, 321), (322, 328), (329, 334), (334, 335), (336, 347), (348, 354), (355, 358), (359, 367), (368, 370), (371, 374), (375, 377), (377, 381), (381, 382), (383, 384), (385, 391), (392, 397), (398, 400), (401, 407), (408, 411), (412, 422), (422, 423), (424, 426), (427, 429), (430, 431), (432, 439), (440, 442), (443, 446), (447, 449), (449, 453), (454, 456), (457, 460), (460, 464), (464, 465), (466, 472), (473, 478), (479, 482), (483, 489), (490, 494), (495, 502), (502, 504), (505, 513), (514, 516), (517, 522), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (71, 72), (0, 0), (281, 284), (285, 289), (290, 298), (299, 301), (302, 305), (306, 314), (315, 317), (318, 321), (322, 328), (329, 334), (334, 335), (336, 347), (348, 354), (355, 358), (359, 367), (368, 370), (371, 374), (375, 377), (377, 381), (381, 382), (383, 384), (385, 391), (392, 397), (398, 400), (401, 407), (408, 411), (412, 422), (422, 423), (424, 426), (427, 429), (430, 431), (432, 439), (440, 442), (443, 446), (447, 449), (449, 453), (454, 456), (457, 460), (460, 464), (464, 465), (466, 472), (473, 478), (479, 482), (483, 489), (490, 494), (495, 502), (502, 504), (505, 513), (514, 516), (517, 522), (523, 527), (527, 530), (530, 533), (534, 536), (536, 538), (538, 541), (541, 543), (544, 546), (547, 551), (551, 552), (553, 555), (556, 559), (560, 563), (564, 566), (567, 570), (571, 575), (576, 581), (582, 583), (584, 587), (588, 590), (591, 592), (593, 599), (600, 604), (605, 609), (610, 618), (619, 626), (627, 628), (629, 636), (637, 640), (641, 644), (645, 649), (650, 654), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (71, 72), (0, 0), (440, 442), (443, 446), (447, 449), (449, 453), (454, 456), (457, 460), (460, 464), (464, 465), (466, 472), (473, 478), (479, 482), (483, 489), (490, 494), (495, 502), (502, 504), (505, 513), (514, 516), (517, 522), (523, 527), (527, 530), (530, 533), (534, 536), (536, 538), (538, 541), (541, 543), (544, 546), (547, 551), (551, 552), (553, 555), (556, 559), (560, 563), (564, 566), (567, 570), (571, 575), (576, 581), (582, 583), (584, 587), (588, 590), (591, 592), (593, 599), (600, 604), (605, 609), (610, 618), (619, 626), (627, 628), (629, 636), (637, 640), (641, 644), (645, 649), (650, 654), (655, 656), (656, 657), (658, 660), (661, 662), (663, 669), (669, 670), (671, 677), (678, 683), (684, 690), (691, 693), (694, 698), (698, 699), (0, 0)]]\n",
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n",
      "[CLS] to whom did the virgin mary allegedly appear in 1858 in lourdes france? [SEP] architecturally, the school has a catholic character. atop the main building ' s gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the gr [SEP]\n"
     ]
    }
   ],
   "source": [
    "question = \"to whom did the virgin mary allegedly appear in 1858 in lourdes france ?\"\n",
    "context = 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary.'\n",
    "answers = {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n",
    "tokens = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    truncation=\"only_second\",\n",
    "    max_length=100,\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,)\n",
    "\n",
    "print(tokens)\n",
    "print(\"inputs_id: \", tokens['input_ids'])\n",
    "print(\"overflow_to_sample_mapping: \", tokens['overflow_to_sample_mapping'])\n",
    "print(\"off_mapping: \", tokens[\"offset_mapping\"])\n",
    "print(tokens.sequence_ids(3))\n",
    "print(tokenizer.decode(tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing train with 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The example gave 4 features.\n",
      "Here is where each comes from: [0, 0, 0, 0].\n",
      "541\n",
      "541\n",
      "541\n",
      "541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0, 0, 65, 33], [0, 0, 72, 40])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dài hơn max_length\n",
    "question = \"to whom did the virgin mary allegedly appear in 1858 in lourdes france ?\"\n",
    "context = 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary.'\n",
    "answers = {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n",
    "\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "print(f\"The example gave {len(inputs['input_ids'])} features.\")\n",
    "print(\n",
    "    f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")\n",
    "\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "# Corrected loop for single example: Iterate through features' offset_mapping\n",
    "for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
    "    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n",
    "    answer = answers  # answers is already a single dict\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "    print(end_char)\n",
    "\n",
    "    # Find the start and end of the context\n",
    "    idx_seq = 0  # Use idx_seq to avoid confusion with feature index 'i'\n",
    "    while sequence_ids[idx_seq] != 1:\n",
    "        idx_seq += 1\n",
    "    context_start = idx_seq\n",
    "    while sequence_ids[idx_seq] == 1:\n",
    "        idx_seq += 1\n",
    "    context_end = idx_seq - 1\n",
    "\n",
    "    # If the answer is not fully inside the context, label is (0, 0)\n",
    "    if context_start >= len(offset) or context_end >= len(offset) or offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "        start_positions.append(0)\n",
    "        end_positions.append(0)\n",
    "    else:\n",
    "        # Otherwise it's the start and end token positions\n",
    "        idx_token = context_start  # Use idx_token to iterate over tokens in context\n",
    "        while idx_token <= context_end and offset[idx_token][0] <= start_char:\n",
    "            idx_token += 1\n",
    "        start_positions.append(idx_token - 1)\n",
    "\n",
    "        idx_token = context_end\n",
    "        while idx_token >= context_start and offset[idx_token][1] >= end_char:\n",
    "            idx_token -= 1\n",
    "        end_positions.append(idx_token + 1)\n",
    "\n",
    "start_positions, end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical answer: Saint Bernadette Soubirous, labels give: to saint bernadette soubiro\n"
     ]
    }
   ],
   "source": [
    "idx = 3\n",
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
    "answer = answers[\"text\"][0]\n",
    "\n",
    "start = start_positions[idx]\n",
    "end = end_positions[idx]\n",
    "labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start: end + 1])\n",
    "\n",
    "print(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing train with mini-batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 4 examples gave 17 features.\n",
      "Here is where each comes from: [0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([81, 49, 17, 0, 0, 57, 19, 33, 0, 0, 0, 63, 27, 0, 0, 0, 0],\n",
       " [83, 51, 19, 0, 0, 63, 25, 39, 0, 0, 0, 64, 28, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    raw_datasets[\"train\"][2:6][\"question\"],\n",
    "    raw_datasets[\"train\"][2:6][\"context\"],\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n",
    "print(\n",
    "    f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")\n",
    "\n",
    "answers = raw_datasets[\"train\"][2:6][\"answers\"]\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
    "    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n",
    "    answer = answers[sample_idx]\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # Find the start and end of the context\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "        idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1:\n",
    "        idx += 1\n",
    "    context_end = idx - 1\n",
    "\n",
    "    # If the answer is not fully inside the context, label is (0, 0)\n",
    "    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "        start_positions.append(0)\n",
    "        end_positions.append(0)\n",
    "    else:\n",
    "        # Otherwise it's the start and end token positions\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "start_positions, end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical answer: a Marian place of prayer and reflection, labels give: [CLS]\n"
     ]
    }
   ],
   "source": [
    "idx = 4\n",
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
    "answer = answers[sample_idx][\"text\"][0]\n",
    "\n",
    "start = start_positions[idx]\n",
    "end = end_positions[idx]\n",
    "labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start: end + 1])\n",
    "\n",
    "print(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing validation 1 sample and mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Input Keys: dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'example_id'])\n",
      "Input IDs (first feature): [[101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 6549, 2135, 1010, 1996, 2082, 2038, 1037, 3234, 2839, 1012, 10234, 1996, 2364, 2311, 1005, 1055, 2751, 8514, 2003, 1037, 3585, 6231, 1997, 1996, 6261, 2984, 1012, 3202, 1999, 2392, 1997, 1996, 2364, 2311, 1998, 5307, 2009, 1010, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 6924, 2007, 1996, 5722, 1000, 2310, 3490, 2618, 4748, 2033, 18168, 5267, 1000, 1012, 2279, 2000, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 102], [101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 2364, 2311, 1998, 5307, 2009, 1010, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 6924, 2007, 1996, 5722, 1000, 2310, 3490, 2618, 4748, 2033, 18168, 5267, 1000, 1012, 2279, 2000, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 14042, 2173, 1997, 7083, 1998, 9185, 1012, 2009, 2003, 1037, 15059, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 102], [101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 14042, 2173, 1997, 7083, 1998, 9185, 1012, 2009, 2003, 1037, 15059, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271, 1999, 8517, 1012, 2012, 1996, 2203, 1997, 1996, 2364, 3298, 1006, 1998, 1999, 1037, 3622, 2240, 2008, 8539, 2083, 1017, 11342, 1998, 1996, 2751, 8514, 102], [101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271, 1999, 8517, 1012, 2012, 1996, 2203, 1997, 1996, 2364, 3298, 1006, 1998, 1999, 1037, 3622, 2240, 2008, 8539, 2083, 1017, 11342, 1998, 1996, 2751, 8514, 1007, 1010, 2003, 1037, 3722, 1010, 2715, 2962, 6231, 1997, 2984, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "Attention Mask (first feature): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Offset Mapping (first feature): [[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (251, 253), (253, 255), (255, 257), (258, 260), (261, 263), (264, 266), (266, 269), (270, 271), (271, 272), (273, 277), (278, 280), (281, 284), (285, 289), (290, 298), (299, 301), (302, 305), (306, 314), (315, 317), (318, 321), (322, 328), (329, 334), (334, 335), (336, 347), (348, 354), (355, 358), (359, 367), (368, 370), (371, 374), (375, 377), None], [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (251, 253), (253, 255), (255, 257), (258, 260), (261, 263), (264, 266), (266, 269), (270, 271), (271, 272), (273, 277), (278, 280), (281, 284), (285, 289), (290, 298), (299, 301), (302, 305), (306, 314), (315, 317), (318, 321), (322, 328), (329, 334), (334, 335), (336, 347), (348, 354), (355, 358), (359, 367), (368, 370), (371, 374), (375, 377), (377, 381), (381, 382), (383, 384), (385, 391), (392, 397), (398, 400), (401, 407), (408, 411), (412, 422), (422, 423), (424, 426), (427, 429), (430, 431), (432, 439), (440, 442), (443, 446), (447, 449), (449, 453), (454, 456), (457, 460), (460, 464), (464, 465), (466, 472), (473, 478), (479, 482), (483, 489), (490, 494), (495, 502), (502, 504), (505, 513), (514, 516), (517, 522), None], [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, (281, 284), (285, 289), (290, 298), (299, 301), (302, 305), (306, 314), (315, 317), (318, 321), (322, 328), (329, 334), (334, 335), (336, 347), (348, 354), (355, 358), (359, 367), (368, 370), (371, 374), (375, 377), (377, 381), (381, 382), (383, 384), (385, 391), (392, 397), (398, 400), (401, 407), (408, 411), (412, 422), (422, 423), (424, 426), (427, 429), (430, 431), (432, 439), (440, 442), (443, 446), (447, 449), (449, 453), (454, 456), (457, 460), (460, 464), (464, 465), (466, 472), (473, 478), (479, 482), (483, 489), (490, 494), (495, 502), (502, 504), (505, 513), (514, 516), (517, 522), (523, 527), (527, 530), (530, 533), (534, 536), (536, 538), (538, 541), (541, 543), (544, 546), (547, 551), (551, 552), (553, 555), (556, 559), (560, 563), (564, 566), (567, 570), (571, 575), (576, 581), (582, 583), (584, 587), (588, 590), (591, 592), (593, 599), (600, 604), (605, 609), (610, 618), (619, 626), (627, 628), (629, 636), (637, 640), (641, 644), (645, 649), (650, 654), None], [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, (440, 442), (443, 446), (447, 449), (449, 453), (454, 456), (457, 460), (460, 464), (464, 465), (466, 472), (473, 478), (479, 482), (483, 489), (490, 494), (495, 502), (502, 504), (505, 513), (514, 516), (517, 522), (523, 527), (527, 530), (530, 533), (534, 536), (536, 538), (538, 541), (541, 543), (544, 546), (547, 551), (551, 552), (553, 555), (556, 559), (560, 563), (564, 566), (567, 570), (571, 575), (576, 581), (582, 583), (584, 587), (588, 590), (591, 592), (593, 599), (600, 604), (605, 609), (610, 618), (619, 626), (627, 628), (629, 636), (637, 640), (641, 644), (645, 649), (650, 654), (655, 656), (656, 657), (658, 660), (661, 662), (663, 669), (669, 670), (671, 677), (678, 683), (684, 690), (691, 693), (694, 698), (698, 699), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]]\n",
      "Example ID: ['56be85543aeaaa14008c9063', '56be85543aeaaa14008c9063', '56be85543aeaaa14008c9063', '56be85543aeaaa14008c9063']\n"
     ]
    }
   ],
   "source": [
    "question = \"to whom did the virgin mary allegedly appear in 1858 in lourdes france ?\"\n",
    "context = 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary.'\n",
    "answers = {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n",
    "\n",
    "max_length = 100\n",
    "stride = 50\n",
    "\n",
    "def preprocess_single_example(question, context, example_id):\n",
    "    \"\"\"\n",
    "    Preprocesses a single question-context example for validation, similar to preprocess_validation_examples.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question string.\n",
    "        context (str): The context string.\n",
    "        example_id (str or int): A unique identifier for the example.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the preprocessed inputs, including input_ids,\n",
    "              attention_mask, offset_mapping, and example_id.\n",
    "    \"\"\"\n",
    "    questions = [question.strip()]  # Wrap question in a list\n",
    "    contexts = [context]  # Wrap context in a list\n",
    "    examples = {\"question\": questions, \"context\": contexts,\n",
    "                \"id\": [example_id]}  # Simulate examples batch\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,  # Use the list of questions\n",
    "        contexts,  # Use the list of contexts\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",  # Keep padding for consistency\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# You can use any identifier for your single example\n",
    "example_id = \"56be85543aeaaa14008c9063\"\n",
    "processed_input = preprocess_single_example(question, context, example_id)\n",
    "\n",
    "print(\"Processed Input Keys:\", processed_input.keys())\n",
    "print(\"Input IDs (first feature):\", processed_input[\"input_ids\"])\n",
    "print(\"Attention Mask (first feature):\", processed_input[\"attention_mask\"][0])\n",
    "print(\"Offset Mapping (first feature):\", processed_input[\"offset_mapping\"])\n",
    "print(\"Example ID:\", processed_input[\"example_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1294.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "small_eval_set = raw_datasets[\"validation\"].select(range(100))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "eval_set = small_eval_set.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "eval_set_for_model.set_format(\"torch\")\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "batch = {k: eval_set_for_model[k].to(device)\n",
    "         for k in eval_set_for_model.column_names}\n",
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = trained_model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ 0.0117, -0.0059,  0.1130,  ..., -0.1730, -0.2052,  0.1237],\n",
       "        [ 0.0970,  0.0544,  0.1646,  ...,  0.1479,  0.0669,  0.2438],\n",
       "        [ 0.1428,  0.0769,  0.2369,  ...,  0.2013,  0.3976,  0.1529],\n",
       "        ...,\n",
       "        [ 0.0635, -0.1360,  0.0261,  ..., -0.0630,  0.0617,  0.1032],\n",
       "        [ 0.1210,  0.0173,  0.0841,  ...,  0.2631,  0.1714,  0.1978],\n",
       "        [ 0.1233,  0.0842,  0.1270,  ..., -0.0102,  0.2096,  0.2625]]), end_logits=tensor([[-0.1218,  0.0535, -0.1136,  ..., -0.2045, -0.0174,  0.0951],\n",
       "        [-0.1907, -0.0134, -0.1944,  ..., -0.3828,  0.0265, -0.0081],\n",
       "        [-0.1893,  0.0307, -0.1950,  ...,  0.3523, -0.1001, -0.0567],\n",
       "        ...,\n",
       "        [-0.1612,  0.0608, -0.1158,  ..., -0.1107, -0.1438, -0.2154],\n",
       "        [-0.1466,  0.1031, -0.3443,  ...,  0.0935,  0.0599, -0.1210],\n",
       "        [-0.1487,  0.0058, -0.1928,  ...,  0.0931,  0.2353,  0.0872]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = outputs.start_logits.cpu().numpy()\n",
    "end_logits = outputs.end_logits.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12367848, -0.20515595, -0.17301556,  0.22297172, -0.14648412,\n",
       "        0.07698998, -0.25346652, -0.45339662, -0.14374146, -0.03400343,\n",
       "       -0.7031467 , -0.40218648, -0.18835765, -0.1797778 , -0.14668193,\n",
       "        0.05877595, -0.45716953, -0.29644898, -0.25350407, -0.05450182],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits[0][-1:-21:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221, 100)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits.shape # từ 100 sample tokenizer ra 366 sample (lý do đã giải thích ở mục preprocesing ở mục train) 100 là maxlength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'56be4db0acb8001400a502ec': [0, 1, 2],\n",
       "             '56be4db0acb8001400a502ed': [3, 4, 5],\n",
       "             '56be4db0acb8001400a502ee': [6, 7, 8],\n",
       "             '56be4db0acb8001400a502ef': [9, 10, 11],\n",
       "             '56be4db0acb8001400a502f0': [12, 13, 14, 15],\n",
       "             '56be8e613aeaaa14008c90d1': [16, 17, 18],\n",
       "             '56be8e613aeaaa14008c90d2': [19, 20, 21],\n",
       "             '56be8e613aeaaa14008c90d3': [22, 23, 24],\n",
       "             '56bea9923aeaaa14008c91b9': [25, 26, 27],\n",
       "             '56bea9923aeaaa14008c91ba': [28, 29, 30],\n",
       "             '56bea9923aeaaa14008c91bb': [31, 32, 33],\n",
       "             '56beace93aeaaa14008c91df': [34, 35, 36],\n",
       "             '56beace93aeaaa14008c91e0': [37, 38, 39],\n",
       "             '56beace93aeaaa14008c91e1': [40, 41, 42],\n",
       "             '56beace93aeaaa14008c91e2': [43, 44, 45, 46],\n",
       "             '56beace93aeaaa14008c91e3': [47, 48, 49],\n",
       "             '56bf10f43aeaaa14008c94fd': [50, 51, 52, 53],\n",
       "             '56bf10f43aeaaa14008c94fe': [54, 55, 56],\n",
       "             '56bf10f43aeaaa14008c94ff': [57, 58, 59],\n",
       "             '56bf10f43aeaaa14008c9500': [60, 61, 62],\n",
       "             '56bf10f43aeaaa14008c9501': [63, 64, 65, 66],\n",
       "             '56d20362e7d4791d009025e8': [67, 68, 69],\n",
       "             '56d20362e7d4791d009025e9': [70, 71, 72],\n",
       "             '56d20362e7d4791d009025ea': [73, 74, 75],\n",
       "             '56d20362e7d4791d009025eb': [76, 77, 78],\n",
       "             '56d600e31c85041400946eae': [79, 80, 81],\n",
       "             '56d600e31c85041400946eb0': [82, 83, 84],\n",
       "             '56d600e31c85041400946eb1': [85, 86, 87],\n",
       "             '56d9895ddc89441400fdb50e': [88, 89, 90],\n",
       "             '56d9895ddc89441400fdb510': [91, 92, 93],\n",
       "             '56be4e1facb8001400a502f6': [94, 95, 96],\n",
       "             '56be4e1facb8001400a502f9': [97, 98, 99],\n",
       "             '56be4e1facb8001400a502fa': [100, 101],\n",
       "             '56beaa4a3aeaaa14008c91c2': [102, 103],\n",
       "             '56beaa4a3aeaaa14008c91c3': [104, 105, 106],\n",
       "             '56bead5a3aeaaa14008c91e9': [107, 108, 109],\n",
       "             '56bead5a3aeaaa14008c91ea': [110, 111, 112],\n",
       "             '56bead5a3aeaaa14008c91eb': [113, 114],\n",
       "             '56bead5a3aeaaa14008c91ec': [115, 116, 117],\n",
       "             '56bead5a3aeaaa14008c91ed': [118, 119],\n",
       "             '56bf159b3aeaaa14008c9507': [120, 121, 122],\n",
       "             '56bf159b3aeaaa14008c9508': [123, 124, 125],\n",
       "             '56bf159b3aeaaa14008c9509': [126, 127, 128],\n",
       "             '56bf159b3aeaaa14008c950a': [129, 130, 131],\n",
       "             '56bf159b3aeaaa14008c950b': [132, 133, 134],\n",
       "             '56d2045de7d4791d009025f3': [135, 136],\n",
       "             '56d2045de7d4791d009025f4': [137, 138, 139],\n",
       "             '56d2045de7d4791d009025f5': [140, 141, 142],\n",
       "             '56d2045de7d4791d009025f6': [143, 144],\n",
       "             '56d6017d1c85041400946ebe': [145, 146, 147],\n",
       "             '56d6017d1c85041400946ec1': [148, 149, 150],\n",
       "             '56d6017d1c85041400946ec2': [151, 152, 153],\n",
       "             '56d98a59dc89441400fdb52a': [154, 155],\n",
       "             '56d98a59dc89441400fdb52b': [156, 157, 158],\n",
       "             '56d98a59dc89441400fdb52e': [159, 160],\n",
       "             '56be4eafacb8001400a50302': [161],\n",
       "             '56be4eafacb8001400a50303': [162],\n",
       "             '56be4eafacb8001400a50304': [163],\n",
       "             '56beab833aeaaa14008c91d2': [164],\n",
       "             '56beab833aeaaa14008c91d3': [165],\n",
       "             '56beab833aeaaa14008c91d4': [166],\n",
       "             '56beae423aeaaa14008c91f4': [167],\n",
       "             '56beae423aeaaa14008c91f5': [168],\n",
       "             '56beae423aeaaa14008c91f6': [169],\n",
       "             '56beae423aeaaa14008c91f7': [170],\n",
       "             '56bf17653aeaaa14008c9511': [171],\n",
       "             '56bf17653aeaaa14008c9513': [172],\n",
       "             '56bf17653aeaaa14008c9514': [173],\n",
       "             '56bf17653aeaaa14008c9515': [174],\n",
       "             '56d204ade7d4791d00902603': [175],\n",
       "             '56d204ade7d4791d00902604': [176],\n",
       "             '56d601e41c85041400946ece': [177],\n",
       "             '56d601e41c85041400946ecf': [178],\n",
       "             '56d601e41c85041400946ed0': [179],\n",
       "             '56d601e41c85041400946ed1': [180],\n",
       "             '56d601e41c85041400946ed2': [181],\n",
       "             '56d98b33dc89441400fdb53b': [182],\n",
       "             '56d98b33dc89441400fdb53c': [183],\n",
       "             '56d98b33dc89441400fdb53d': [184],\n",
       "             '56d98b33dc89441400fdb53e': [185],\n",
       "             '56be5333acb8001400a5030a': [186, 187],\n",
       "             '56be5333acb8001400a5030b': [188, 189],\n",
       "             '56be5333acb8001400a5030c': [190],\n",
       "             '56be5333acb8001400a5030d': [191, 192],\n",
       "             '56be5333acb8001400a5030e': [193, 194],\n",
       "             '56beaf5e3aeaaa14008c91fd': [195, 196],\n",
       "             '56beaf5e3aeaaa14008c91fe': [197, 198],\n",
       "             '56beaf5e3aeaaa14008c91ff': [199, 200],\n",
       "             '56beaf5e3aeaaa14008c9200': [201, 202],\n",
       "             '56beaf5e3aeaaa14008c9201': [203, 204],\n",
       "             '56bf1ae93aeaaa14008c951b': [205],\n",
       "             '56bf1ae93aeaaa14008c951c': [206, 207],\n",
       "             '56bf1ae93aeaaa14008c951e': [208, 209],\n",
       "             '56bf1ae93aeaaa14008c951f': [210, 211],\n",
       "             '56d2051ce7d4791d00902608': [212],\n",
       "             '56d2051ce7d4791d00902609': [213, 214],\n",
       "             '56d2051ce7d4791d0090260a': [215, 216],\n",
       "             '56d2051ce7d4791d0090260b': [217, 218],\n",
       "             '56d602631c85041400946ed8': [219],\n",
       "             '56d602631c85041400946eda': [220]})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "example_to_features = collections.defaultdict(list)\n",
    "for idx, feature in enumerate(eval_set):\n",
    "    example_to_features[feature[\"example_id\"]].append(idx)\n",
    "example_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "predicted_answers = []\n",
    "\n",
    "for example in small_eval_set:\n",
    "    example_id = example[\"id\"]\n",
    "    context = example[\"context\"]\n",
    "    answers = []\n",
    "\n",
    "    for feature_index in example_to_features[example_id]:\n",
    "        start_logit = start_logits[feature_index]\n",
    "        end_logit = end_logits[feature_index]\n",
    "        offsets = eval_set[\"offset_mapping\"][feature_index]\n",
    "\n",
    "        start_indexes = np.argsort(start_logit)[-1: -n_best - 1: -1].tolist()\n",
    "        end_indexes = np.argsort(end_logit)[-1: -n_best - 1: -1].tolist()\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                # Skip answers that are not fully in the context\n",
    "                if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                    continue\n",
    "                # Skip answers with a length that is either < 0 or > max_answer_length.\n",
    "                if (\n",
    "                    end_index < start_index\n",
    "                    or end_index - start_index + 1 > max_answer_length\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                answers.append(\n",
    "                    {\n",
    "                        \"text\": context[offsets[start_index][0]: offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    if len(answers) > 0:\n",
    "        best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "        predicted_answers.append(\n",
    "            {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})\n",
    "    else:\n",
    "        predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "        print(\"rỗng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '56be4db0acb8001400a502ec',\n",
       "  'prediction_text': ') champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56be4db0acb8001400a502ed',\n",
       "  'prediction_text': ') champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56be4db0acb8001400a502ee',\n",
       "  'prediction_text': ') for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56be4db0acb8001400a502ef',\n",
       "  'prediction_text': ') for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56be4db0acb8001400a502f0',\n",
       "  'prediction_text': 'Broncos defeated the National Football Conference ('},\n",
       " {'id': '56be8e613aeaaa14008c90d1',\n",
       "  'prediction_text': '. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56be8e613aeaaa14008c90d2',\n",
       "  'prediction_text': '. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56be8e613aeaaa14008c90d3',\n",
       "  'prediction_text': ') for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56bea9923aeaaa14008c91b9',\n",
       "  'prediction_text': '. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56bea9923aeaaa14008c91ba',\n",
       "  'prediction_text': ') for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56bea9923aeaaa14008c91bb',\n",
       "  'prediction_text': '. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56beace93aeaaa14008c91df',\n",
       "  'prediction_text': ') for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56beace93aeaaa14008c91e0',\n",
       "  'prediction_text': '. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56beace93aeaaa14008c91e1',\n",
       "  'prediction_text': '. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56beace93aeaaa14008c91e2',\n",
       "  'prediction_text': 'Broncos defeated the National Football Conference ('},\n",
       " {'id': '56beace93aeaaa14008c91e3',\n",
       "  'prediction_text': ') champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56bf10f43aeaaa14008c94fd',\n",
       "  'prediction_text': 'Broncos defeated the National Football Conference ('},\n",
       " {'id': '56bf10f43aeaaa14008c94fe',\n",
       "  'prediction_text': '. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56bf10f43aeaaa14008c94ff',\n",
       "  'prediction_text': '. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56bf10f43aeaaa14008c9500',\n",
       "  'prediction_text': '. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56bf10f43aeaaa14008c9501',\n",
       "  'prediction_text': 'Broncos defeated the National Football Conference ('},\n",
       " {'id': '56d20362e7d4791d009025e8',\n",
       "  'prediction_text': ') for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56d20362e7d4791d009025e9',\n",
       "  'prediction_text': ') for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56d20362e7d4791d009025ea',\n",
       "  'prediction_text': ') for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56d20362e7d4791d009025eb',\n",
       "  'prediction_text': ') for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56d600e31c85041400946eae',\n",
       "  'prediction_text': ') champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56d600e31c85041400946eb0',\n",
       "  'prediction_text': ') for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56d600e31c85041400946eb1',\n",
       "  'prediction_text': ') for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56d9895ddc89441400fdb50e',\n",
       "  'prediction_text': '. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56d9895ddc89441400fdb510',\n",
       "  'prediction_text': ') for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference ('},\n",
       " {'id': '56be4e1facb8001400a502f6',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56be4e1facb8001400a502f9',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56be4e1facb8001400a502fa',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56beaa4a3aeaaa14008c91c2',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56beaa4a3aeaaa14008c91c3',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56bead5a3aeaaa14008c91e9',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56bead5a3aeaaa14008c91ea',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56bead5a3aeaaa14008c91eb',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56bead5a3aeaaa14008c91ec',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56bead5a3aeaaa14008c91ed',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56bf159b3aeaaa14008c9507',\n",
       "  'prediction_text': '–18 in the AFC Championship Game.'},\n",
       " {'id': '56bf159b3aeaaa14008c9508',\n",
       "  'prediction_text': '–18 in the AFC Championship Game.'},\n",
       " {'id': '56bf159b3aeaaa14008c9509',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56bf159b3aeaaa14008c950a',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56bf159b3aeaaa14008c950b',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56d2045de7d4791d009025f3',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56d2045de7d4791d009025f4',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56d2045de7d4791d009025f5',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56d2045de7d4791d009025f6',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56d6017d1c85041400946ebe',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56d6017d1c85041400946ec1',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56d6017d1c85041400946ec2',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56d98a59dc89441400fdb52a',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56d98a59dc89441400fdb52b',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56d98a59dc89441400fdb52e',\n",
       "  'prediction_text': 'Pittsburgh Steelers as one of four teams that have made eight appearances in the Super Bowl.'},\n",
       " {'id': '56be4eafacb8001400a50302',\n",
       "  'prediction_text': '. Denver linebacker Von Miller was named Super Bowl MVP, recording five solo tackles, 2½ sacks, and two forced fumbles.'},\n",
       " {'id': '56be4eafacb8001400a50303', 'prediction_text': '.'},\n",
       " {'id': '56be4eafacb8001400a50304', 'prediction_text': '.'},\n",
       " {'id': '56beab833aeaaa14008c91d2',\n",
       "  'prediction_text': '. Denver linebacker Von Miller was named Super Bowl MVP, recording five solo tackles, 2½ sacks, and two forced fumbles.'},\n",
       " {'id': '56beab833aeaaa14008c91d3', 'prediction_text': '.'},\n",
       " {'id': '56beab833aeaaa14008c91d4',\n",
       "  'prediction_text': '. Denver linebacker Von Miller was named Super Bowl MVP, recording five solo tackles, 2½ sacks, and two forced fumbles.'},\n",
       " {'id': '56beae423aeaaa14008c91f4',\n",
       "  'prediction_text': '. Denver linebacker Von Miller was named Super Bowl MVP, recording five solo tackles, 2½ sacks, and two forced fumbles.'},\n",
       " {'id': '56beae423aeaaa14008c91f5',\n",
       "  'prediction_text': '. Denver linebacker Von Miller was named Super Bowl MVP, recording five solo tackles, 2½ sacks, and two forced fumbles.'},\n",
       " {'id': '56beae423aeaaa14008c91f6',\n",
       "  'prediction_text': '. Denver linebacker Von Miller was named Super Bowl MVP, recording five solo tackles, 2½ sacks, and two forced fumbles.'},\n",
       " {'id': '56beae423aeaaa14008c91f7',\n",
       "  'prediction_text': '. Denver linebacker Von Miller was named Super Bowl MVP, recording five solo tackles, 2½ sacks, and two forced fumbles.'},\n",
       " {'id': '56bf17653aeaaa14008c9511', 'prediction_text': '.'},\n",
       " {'id': '56bf17653aeaaa14008c9513', 'prediction_text': '.'},\n",
       " {'id': '56bf17653aeaaa14008c9514', 'prediction_text': '.'},\n",
       " {'id': '56bf17653aeaaa14008c9515', 'prediction_text': '.'},\n",
       " {'id': '56d204ade7d4791d00902603',\n",
       "  'prediction_text': '. Denver linebacker Von Miller was named Super Bowl MVP, recording five solo tackles, 2½ sacks, and two forced fumbles.'},\n",
       " {'id': '56d204ade7d4791d00902604', 'prediction_text': '.'},\n",
       " {'id': '56d601e41c85041400946ece', 'prediction_text': '.'},\n",
       " {'id': '56d601e41c85041400946ecf', 'prediction_text': '.'},\n",
       " {'id': '56d601e41c85041400946ed0',\n",
       "  'prediction_text': '. Denver linebacker Von Miller was named Super Bowl MVP, recording five solo tackles, 2½ sacks, and two forced fumbles.'},\n",
       " {'id': '56d601e41c85041400946ed1', 'prediction_text': '.'},\n",
       " {'id': '56d601e41c85041400946ed2',\n",
       "  'prediction_text': '. Denver linebacker Von Miller was named Super Bowl MVP, recording five solo tackles, 2½ sacks, and two forced fumbles.'},\n",
       " {'id': '56d98b33dc89441400fdb53b', 'prediction_text': '.'},\n",
       " {'id': '56d98b33dc89441400fdb53c', 'prediction_text': '.'},\n",
       " {'id': '56d98b33dc89441400fdb53d', 'prediction_text': '.'},\n",
       " {'id': '56d98b33dc89441400fdb53e', 'prediction_text': '.'},\n",
       " {'id': '56be5333acb8001400a5030a',\n",
       "  'prediction_text': 'Bowl 50 halftime show was headlined by the British rock'},\n",
       " {'id': '56be5333acb8001400a5030b',\n",
       "  'prediction_text': 'Bowl 50 halftime show was headlined by the British rock'},\n",
       " {'id': '56be5333acb8001400a5030c',\n",
       "  'prediction_text': 'Bowl 50 halftime show was headlined by the British rock'},\n",
       " {'id': '56be5333acb8001400a5030d',\n",
       "  'prediction_text': 'Bowl 50 halftime show was headlined by the British rock'},\n",
       " {'id': '56be5333acb8001400a5030e', 'prediction_text': 'by the British rock'},\n",
       " {'id': '56beaf5e3aeaaa14008c91fd',\n",
       "  'prediction_text': 'Bowl 50 halftime show was headlined by the British rock'},\n",
       " {'id': '56beaf5e3aeaaa14008c91fe', 'prediction_text': 'by the British rock'},\n",
       " {'id': '56beaf5e3aeaaa14008c91ff', 'prediction_text': 'by the British rock'},\n",
       " {'id': '56beaf5e3aeaaa14008c9200', 'prediction_text': 'by the British rock'},\n",
       " {'id': '56beaf5e3aeaaa14008c9201', 'prediction_text': 'by the British rock'},\n",
       " {'id': '56bf1ae93aeaaa14008c951b', 'prediction_text': 'by the British rock'},\n",
       " {'id': '56bf1ae93aeaaa14008c951c',\n",
       "  'prediction_text': 'Bowl 50 halftime show was headlined by the British rock'},\n",
       " {'id': '56bf1ae93aeaaa14008c951e',\n",
       "  'prediction_text': 'Bowl 50 halftime show was headlined by the British rock'},\n",
       " {'id': '56bf1ae93aeaaa14008c951f',\n",
       "  'prediction_text': 'Bowl 50 halftime show was headlined by the British rock'},\n",
       " {'id': '56d2051ce7d4791d00902608', 'prediction_text': 'by the British rock'},\n",
       " {'id': '56d2051ce7d4791d00902609', 'prediction_text': 'by the British rock'},\n",
       " {'id': '56d2051ce7d4791d0090260a', 'prediction_text': 'by the British rock'},\n",
       " {'id': '56d2051ce7d4791d0090260b',\n",
       "  'prediction_text': '. The Super Bowl 50 halftime show was headlined by the British rock'},\n",
       " {'id': '56d602631c85041400946ed8', 'prediction_text': 'by the British rock'},\n",
       " {'id': '56d602631c85041400946eda', 'prediction_text': 'by the British rock'}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '56be4db0acb8001400a502ec', 'prediction_text': ') champion Denver Broncos defeated the National Football Conference ('}\n",
      "{'id': '56be4db0acb8001400a502ec', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "theoretical_answers = [\n",
    "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n",
    "]\n",
    "\n",
    "print(predicted_answers[0])\n",
    "print(theoretical_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.0, 'f1': 4.282815974874798}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completed postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 572.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.0, 'f1': 4.282815974874798}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(\n",
    "                start_logit)[-1: -n_best - 1: -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1: -n_best - 1: -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0]: offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [\n",
    "        {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)\n",
    "\n",
    "\n",
    "start_logits = outputs.start_logits.cpu().numpy()\n",
    "end_logits = outputs.end_logits.cpu().numpy()\n",
    "compute_metrics(start_logits, end_logits, eval_set, small_eval_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIOEx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
