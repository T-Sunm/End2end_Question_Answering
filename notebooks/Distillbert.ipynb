{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import bibraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qq datasets==2.16.1 evaluate==0.4.1 transformers[sentencepiece]==4.35.2\n",
    "# !pip install -qq accelerate==0.26.1\n",
    "# !apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Asus\\Ungdung\\Miniconda\\workspace\\envs\\AIOEx\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import evaluate\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sử dụng mô hình \"distilbert-base-uncased\"\n",
    "# làm mô hình checkpoint\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "# Độ dài tối đa cho mỗi đoạn văn bản\n",
    "# sau khi được xử lý\n",
    "MAX_LENGTH = 384\n",
    "\n",
    "# Khoảng cách giữa các điểm bắt đầu\n",
    "# của các đoạn văn bản liên tiếp\n",
    "STRIDE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"rajpurkar/squad_v2\"\n",
    "raw_datasets = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 130319\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 11873\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56be85543aeaaa14008c9066',\n",
       " 'title': 'Beyoncé',\n",
       " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
       " 'question': \"When did Beyonce leave Destiny's Child and become a solo singer?\",\n",
       " 'answers': {'text': ['2003'], 'answer_start': [526]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56ddde6b9a695914005b9628',\n",
       " 'title': 'Normans',\n",
       " 'context': 'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.',\n",
       " 'question': 'In what country is Normandy located?',\n",
       " 'answers': {'text': ['France', 'France', 'France', 'France'],\n",
       "  'answer_start': [159, 159, 159, 159]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['validation'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lưu ý khi tokenize\n",
    "truncation (Cắt bớt):\n",
    "\n",
    "Mục đích: Quyết định có cắt bớt chuỗi đầu vào hay không, và nếu có thì cắt như thế nào. Hầu hết các mô hình transformer (như BERT, RoBERTa,...) có giới hạn độ dài đầu vào (ví dụ: 512 token). truncation giúp xử lý các chuỗi dài hơn giới hạn này.\n",
    "\n",
    "Giá trị:\n",
    "\n",
    "- False hoặc 'do_not_truncate' (mặc định): Không cắt bớt. Nếu chuỗi dài hơn giới hạn của mô hình, bạn có thể gặp lỗi.\n",
    "\n",
    "- True hoặc 'longest_first': Cắt bớt theo nguyên tắc \"dài nhất trước\".\n",
    "\n",
    "    - Nếu bạn đưa vào một chuỗi: Cắt chuỗi đó cho đến khi nó đạt độ dài tối đa (max_length hoặc độ dài tối đa của mô hình).\n",
    "\n",
    "    - Nếu bạn đưa vào một cặp chuỗi (ví dụ: câu hỏi và ngữ cảnh): Cắt chuỗi dài hơn trong cặp cho đến khi tổng độ dài của cả hai chuỗi (cộng thêm các token đặc biệt như [CLS], [SEP]) đạt max_length. Tiếp tục cắt chuỗi dài hơn cho đến khi đạt được độ dài mong muốn.\n",
    "\n",
    "- 'only_first': Chỉ cắt chuỗi thứ nhất trong cặp chuỗi (hoặc batch). Chuỗi thứ hai không bị ảnh hưởng.\n",
    "\n",
    "- 'only_second': Chỉ cắt chuỗi thứ hai trong cặp chuỗi (hoặc batch). Chuỗi thứ nhất không bị ảnh hưởng.\n",
    "\n",
    "max_length (Độ dài tối đa):\n",
    "\n",
    "- Mục đích: Đặt giới hạn độ dài tuyệt đối cho chuỗi đầu ra (sau khi đã cắt bớt, nếu có).\n",
    "\n",
    "- Giá trị:\n",
    "\n",
    "    - int: Một số nguyên dương chỉ định độ dài tối đa.\n",
    "\n",
    "    - None (mặc định): Nếu truncation được bật, max_length sẽ tự động được đặt bằng độ dài tối đa mà mô hình hỗ trợ (ví dụ: 512 với BERT). Nếu truncation tắt, max_length không có tác dụng.\n",
    "\n",
    "- Quan trọng: max_length bao gồm cả các token đặc biệt ([CLS], [SEP], ...).\n",
    "\n",
    "stride (Bước nhảy):\n",
    "\n",
    "- Mục đích: Chỉ có tác dụng khi truncation=True và return_overflowing_tokens=True. Khi một chuỗi bị cắt bớt, stride cho phép bạn tạo ra các \"cửa sổ trượt\" (sliding windows) chồng lấn lên nhau, giúp giữ lại một phần ngữ cảnh từ phần bị cắt.\n",
    "\n",
    "- Giá trị:\n",
    "    - int : Số token được giữ lại (overlap) từ phần cuối của chuỗi đã cắt, và đưa vào đầu của chuỗi \"overflowing\" tiếp theo.\n",
    "\n",
    "return_offsets_mapping là một tham số tùy chọn (mặc định là False) trong các hàm tokenizer của Hugging Face Transformers, nó quyết định xem có trả về thông tin về vị trí ký tự (character-level) của mỗi token trong chuỗi gốc hay không.\n",
    "- Offsets: \"Offsets\" là một cặp số (char_start, char_end):\n",
    "    - char_start: Vị trí ký tự bắt đầu của token trong chuỗi gốc.\n",
    "    - char_end: Vị trí ký tự kết thúc của token trong chuỗi gốc (lưu ý: ký tự tại char_end không thuộc token). Nói cách khác, khoảng [char_start, char_end) chứa token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2023, 2003, 1037, 2200, 2146, 6251, 1012, 102, 20773, 21025, 19358, 22815, 1011, 102], [101, 2023, 2003, 1037, 2200, 2146, 6251, 1012, 102, 22815, 1011, 5708, 1006, 1013, 102], [101, 2023, 2003, 1037, 2200, 2146, 6251, 1012, 102, 1006, 1013, 12170, 23432, 29715, 102], [101, 2023, 2003, 1037, 2200, 2146, 6251, 1012, 102, 23432, 29715, 3501, 29678, 12325, 102], [101, 2023, 2003, 1037, 2200, 2146, 6251, 1012, 102, 29678, 12325, 29685, 1013, 10506, 102], [101, 2023, 2003, 1037, 2200, 2146, 6251, 1012, 102, 1013, 10506, 1011, 10930, 2078, 102], [101, 2023, 2003, 1037, 2200, 2146, 6251, 1012, 102, 10930, 2078, 1011, 2360, 1007, 102], [101, 2023, 2003, 1037, 2200, 2146, 6251, 1012, 102, 2360, 1007, 1006, 2141, 2244, 102], [101, 2023, 2003, 1037, 2200, 2146, 6251, 1012, 102, 2141, 2244, 1018, 1010, 3261, 102], [101, 2023, 2003, 1037, 2200, 2146, 6251, 1012, 102, 1010, 3261, 1007, 1012, 102, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], 'offset_mapping': [[(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (0, 7), (8, 10), (10, 15), (16, 23), (23, 24), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (16, 23), (23, 24), (24, 30), (31, 32), (32, 33), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (31, 32), (32, 33), (33, 35), (35, 36), (36, 37), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (35, 36), (36, 37), (37, 38), (38, 39), (39, 42), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (38, 39), (39, 42), (42, 43), (43, 44), (45, 48), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (43, 44), (45, 48), (48, 49), (49, 51), (51, 52), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (49, 51), (51, 52), (52, 53), (53, 56), (56, 57), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (53, 56), (56, 57), (58, 59), (59, 63), (64, 73), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (59, 63), (64, 73), (74, 75), (75, 76), (77, 81), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (75, 76), (77, 81), (81, 82), (82, 83), (0, 0), (0, 0)]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "length inputs_id:  10\n",
      "length overflow_to_sample_mapping:  10\n",
      "length offset_mapping, off_mapping:  10 ,  [[(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (0, 7), (8, 10), (10, 15), (16, 23), (23, 24), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (16, 23), (23, 24), (24, 30), (31, 32), (32, 33), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (31, 32), (32, 33), (33, 35), (35, 36), (36, 37), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (35, 36), (36, 37), (37, 38), (38, 39), (39, 42), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (38, 39), (39, 42), (42, 43), (43, 44), (45, 48), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (43, 44), (45, 48), (48, 49), (49, 51), (51, 52), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (49, 51), (51, 52), (52, 53), (53, 56), (56, 57), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (53, 56), (56, 57), (58, 59), (59, 63), (64, 73), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (59, 63), (64, 73), (74, 75), (75, 76), (77, 81), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 9), (10, 14), (15, 19), (20, 28), (28, 29), (0, 0), (75, 76), (77, 81), (81, 82), (82, 83), (0, 0), (0, 0)]]\n",
      "[None, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, None]\n",
      "[CLS] this is a very long sentence. [SEP] beyonce giselle knowles - [SEP]\n"
     ]
    }
   ],
   "source": [
    "## Test tokenizer\n",
    "text1 = \"This is a very long sentence.\"  # Dài hơn max_length\n",
    "text2 = \"Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981).\"\n",
    "\n",
    "tokens = tokenizer(\n",
    "    text1,\n",
    "    text2,\n",
    "    truncation=\"only_second\",\n",
    "    max_length=15,\n",
    "    stride=2,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\")\n",
    "\n",
    "print(tokens)\n",
    "print(\"length inputs_id: \", len(tokens['input_ids']))\n",
    "print(\"length overflow_to_sample_mapping: \",\n",
    "      len(tokens['overflow_to_sample_mapping']))\n",
    "print(\"length offset_mapping, off_mapping: \", len(tokens[\"offset_mapping\"]) , \", \" , tokens[\"offset_mapping\"])\n",
    "print(tokens.sequence_ids(2))\n",
    "print(tokenizer.decode(tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_start_end(inputs, examples, offset_mapping, sample_map):\n",
    "    # Trích xuất thông tin về câu trả lời (answers) từ examples.\n",
    "    answers = examples[\"answers\"]\n",
    "\n",
    "    # Khởi tạo danh sách các vị trí bắt đầu và kết thúc câu trả lời.\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # Duyệt qua danh sách offset_mapping.\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "\n",
    "        # Trích xuất sequence_ids.\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # xác định start-end id of context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        start_idx = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        end_idx = idx - 1\n",
    "\n",
    "        # Trích xuất thông tin về câu trả lời cho mẫu này\n",
    "        answer = answers[sample_idx]\n",
    "\n",
    "        # Nếu không có câu trả lời, gán nhãn (0, 0).\n",
    "        if len(answer['text']) == 0:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Xác định vị trí ký tự bắt đầu và kết thúc của câu trả lời trong ngữ cảnh.\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "\n",
    "            # Nếu câu trả lời không nằm hoàn toàn trong ngữ cảnh, gán nhãn là (0, 0).\n",
    "            if offset[start_idx][0] > start_char or offset[end_idx][1] < end_char:\n",
    "                start_positions.append(0)\n",
    "                end_positions.append(0)\n",
    "            else:\n",
    "                # Nếu không, xác định vị trí token bắt đầu và kết thúc của câu trả lời.\n",
    "                idx = start_idx\n",
    "                while idx <= end_idx and offset[idx][0] <= start_char:\n",
    "                    idx += 1\n",
    "                start_positions.append(idx - 1)\n",
    "\n",
    "                idx = end_idx\n",
    "                while idx >= start_idx and offset[idx][1] >= end_char:\n",
    "                    idx -= 1\n",
    "                end_positions.append(idx + 1)\n",
    "\n",
    "    return start_positions, end_positions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa hàm preprocess_training_examples và nhận đối số examples là dữ liệu đào tạo\n",
    "def preprocess_training_examples(examples): # batch of samples\n",
    "    inputs = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        stride=STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\")\n",
    "    # Trích xuất offset_mapping và sample_map, loại bỏ chúng khỏi inputs.\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    \n",
    "    start_positions, end_positions = find_start_end(\n",
    "        inputs, examples, offset_mapping, sample_map)\n",
    "\n",
    "    # Thêm thông tin vị trí bắt đầu và kết thúc vào inputs.\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 130319/130319 [00:39<00:00, 3262.78 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(130319, 131754)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tạo một biến train_dataset và gán cho nó giá trị sau khi áp dụng hàm preprocess_training_examples lên tập dữ liệu \"train\"\n",
    "# Bật chế độ xử lý theo từng batch bằng cách đặt batched=True\n",
    "# Loại bỏ các cột không cần thiết trong tập dữ liệu \"train\" bằng cách sử dụng remove_columns\n",
    "\n",
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "# In ra độ dài của tập dữ liệu \"train\" ban đầu và độ dài của tập dữ liệu đã được xử lý (train_dataset)\n",
    "len(raw_datasets[\"train\"]), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa hàm preprocess_training_examples và nhận đối số examples là dữ liệu đào tạo\n",
    "def preprocess_validation_examples(examples):  # batch of samples\n",
    "    inputs = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        stride=STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\")\n",
    "    # Lấy ánh xạ để ánh xạ lại ví dụ tham chiếu cho từng dòng trong inputs\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    # Xác định ví dụ tham chiếu cho mỗi dòng đầu vào và\n",
    "    # điều chỉnh ánh xạ offset\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "\n",
    "        # Loại bỏ các offset không phù hợp với sequence_ids (chỉ giữ lại offset của context)\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None\n",
    "            for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    # Thêm thông tin ví dụ tham chiếu vào đầu vào\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 11873/11873 [00:05<00:00, 2170.47 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11873, 12134)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tạo một biến validation_dataset và gán giá trị bằng việc sử dụng dữ liệu từ raw_datasets[\"validation\"] sau khi áp dụng một hàm xử lý trước.\n",
    "validation_dataset = raw_datasets[\"validation\"].map(\n",
    "    # Gọi hàm preprocess_validation_examples để xử lý dữ liệu đầu vào.\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,  # Xử lý dữ liệu theo từng batch.\n",
    "    # Loại bỏ các cột không cần thiết từ dữ liệu ban đầu.\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "\n",
    "# In ra độ dài của raw_datasets[\"validation\"] và validation_dataset để so sánh.\n",
    "len(raw_datasets[\"validation\"]), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/distilbert-finetuned-squadv2\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=None,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"eval_accuracy\"\n",
    ")\n",
    "\n",
    "# Khởi tạo một đối tượng Trainer để huấn luyện mô hình\n",
    "trainer = Trainer(\n",
    "    model=model,  # Sử dụng mô hình đã tạo trước đó\n",
    "    args=training_args,  # Các tham số và cấu hình huấn luyện\n",
    "    train_dataset=train_dataset,  # Sử dụng tập dữ liệu huấn luyện\n",
    "    eval_dataset=validation_dataset,  # Sử dụng tập dữ liệu đánh giá\n",
    "    tokenizer=tokenizer,  # Sử dụng tokenizer để xử lý văn bản\n",
    ")\n",
    "\n",
    "# Bắt đầu quá trình huấn luyện\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIOEx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
