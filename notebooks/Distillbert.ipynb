{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and import bibraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qq datasets==2.16.1 evaluate==0.4.1 transformers[sentencepiece]==4.35.2\n",
    "# !pip install -qq accelerate==0.26.1\n",
    "# !apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import evaluate\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sử dụng mô hình \"distilbert-base-uncased\"\n",
    "# làm mô hình checkpoint\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "# Độ dài tối đa cho mỗi đoạn văn bản\n",
    "# sau khi được xử lý\n",
    "MAX_LENGTH = 384\n",
    "\n",
    "# Khoảng cách giữa các điểm bắt đầu\n",
    "# của các đoạn văn bản liên tiếp\n",
    "STRIDE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"rajpurkar/squad_v2\"\n",
    "raw_datasets = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lưu ý khi tokenize\n",
    "truncation (Cắt bớt):\n",
    "\n",
    "Mục đích: Quyết định có cắt bớt chuỗi đầu vào hay không, và nếu có thì cắt như thế nào. Hầu hết các mô hình transformer (như BERT, RoBERTa,...) có giới hạn độ dài đầu vào (ví dụ: 512 token). truncation giúp xử lý các chuỗi dài hơn giới hạn này.\n",
    "\n",
    "Giá trị:\n",
    "\n",
    "- False hoặc 'do_not_truncate' (mặc định): Không cắt bớt. Nếu chuỗi dài hơn giới hạn của mô hình, bạn có thể gặp lỗi.\n",
    "\n",
    "- True hoặc 'longest_first': Cắt bớt theo nguyên tắc \"dài nhất trước\".\n",
    "\n",
    "    - Nếu bạn đưa vào một chuỗi: Cắt chuỗi đó cho đến khi nó đạt độ dài tối đa (max_length hoặc độ dài tối đa của mô hình).\n",
    "\n",
    "    - Nếu bạn đưa vào một cặp chuỗi (ví dụ: câu hỏi và ngữ cảnh): Cắt chuỗi dài hơn trong cặp cho đến khi tổng độ dài của cả hai chuỗi (cộng thêm các token đặc biệt như [CLS], [SEP]) đạt max_length. Tiếp tục cắt chuỗi dài hơn cho đến khi đạt được độ dài mong muốn.\n",
    "\n",
    "- 'only_first': Chỉ cắt chuỗi thứ nhất trong cặp chuỗi (hoặc batch). Chuỗi thứ hai không bị ảnh hưởng.\n",
    "\n",
    "- 'only_second': Chỉ cắt chuỗi thứ hai trong cặp chuỗi (hoặc batch). Chuỗi thứ nhất không bị ảnh hưởng.\n",
    "\n",
    "max_length (Độ dài tối đa):\n",
    "\n",
    "- Mục đích: Đặt giới hạn độ dài tuyệt đối cho chuỗi đầu ra (sau khi đã cắt bớt, nếu có).\n",
    "\n",
    "- Giá trị:\n",
    "\n",
    "    - int: Một số nguyên dương chỉ định độ dài tối đa.\n",
    "\n",
    "    - None (mặc định): Nếu truncation được bật, max_length sẽ tự động được đặt bằng độ dài tối đa mà mô hình hỗ trợ (ví dụ: 512 với BERT). Nếu truncation tắt, max_length không có tác dụng.\n",
    "\n",
    "- Quan trọng: max_length bao gồm cả các token đặc biệt ([CLS], [SEP], ...).\n",
    "\n",
    "stride (Bước nhảy):\n",
    "\n",
    "- Mục đích: Chỉ có tác dụng khi truncation=True và return_overflowing_tokens=True. Khi một chuỗi bị cắt bớt, stride cho phép bạn tạo ra các \"cửa sổ trượt\" (sliding windows) chồng lấn lên nhau, giúp giữ lại một phần ngữ cảnh từ phần bị cắt.\n",
    "\n",
    "- Giá trị:\n",
    "    - int : Số token được giữ lại (overlap) từ phần cuối của chuỗi đã cắt, và đưa vào đầu của chuỗi \"overflowing\" tiếp theo.\n",
    "\n",
    "return_offsets_mapping là một tham số tùy chọn (mặc định là False) trong các hàm tokenizer của Hugging Face Transformers, nó quyết định xem có trả về thông tin về vị trí ký tự (character-level) của mỗi token trong chuỗi gốc hay không.\n",
    "- Offsets: \"Offsets\" là một cặp số (char_start, char_end):\n",
    "    - char_start: Vị trí ký tự bắt đầu của token trong chuỗi gốc.\n",
    "    - char_end: Vị trí ký tự kết thúc của token trong chuỗi gốc (lưu ý: ký tự tại char_end không thuộc token). Nói cách khác, khoảng [char_start, char_end) chứa token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa hàm preprocess_training_examples và nhận đối số examples là dữ liệu đào tạo\n",
    "def preprocess_training_examples(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples[\"question\"], \n",
    "        examples[\"context\"], \n",
    "        truncation=\"only_second\", \n",
    "        max_length=MAX_LENGTH, \n",
    "        stride=STRIDE, \n",
    "        return_overflowing_tokens=True, \n",
    "        return_offsets_mapping=True, \n",
    "        padding=\"max_length\")\n",
    "    print(tokens)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test tokenizer\n",
    "text1 = \"This is a very long sentence.\"  # Dài hơn max_length\n",
    "text2 = \"Short.\"\n",
    "\n",
    "tokens = tokenizer(\n",
    "    text1,\n",
    "    text2,\n",
    "    truncation=\"only_second\",\n",
    "    max_length=15,\n",
    "    stride=2,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\")\n",
    "\n",
    "print(tokens)\n",
    "print(tokens.sequence_ids(0))\n",
    "print(tokenizer.decode(tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets.map(preprocess_training_examples, batched=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIOEx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
